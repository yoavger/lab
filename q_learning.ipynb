{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sowftmax(beta, q):\n",
    "    return (np.exp(q * beta)) / sum(np.exp(q * beta))\n",
    "\n",
    "def prediction_error(a , b): return a-b\n",
    "\n",
    "def update_q_table(q_table,action,reward,alpha):\n",
    "    q_table[action] = q_table[action] + (alpha * (prediction_error(reward,q_table[action])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "# set 1 to pritn every trail \n",
    "PRINT_EACH_TRAIL = 0\n",
    "\n",
    "# set num of trails\n",
    "num_of_trails = 2000\n",
    "\n",
    "# set num of action \n",
    "action_space = 2\n",
    "\n",
    "# create the q_table \n",
    "q_table = np.zeros(action_space)\n",
    "\n",
    "# set Learning Rate -  α  \n",
    "alpha = 0.2\n",
    "\n",
    "# set Discounting Factor -  γ \n",
    "gamma = 1\n",
    "\n",
    "# set beta - temperature - β (sowftMax)\n",
    "beta = 5\n",
    "\n",
    "# probabilty for a reward for each action in stage 2\n",
    "prob_reward = np.array([[.9,.1],\n",
    "                        [.1,.9]])\n",
    "\n",
    "reward_list = np.zeros(num_of_trails)\n",
    "\n",
    "action_list = np.zeros(num_of_trails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(num_of_trails):\n",
    "    \n",
    "    if PRINT_EACH_TRAIL: print(\"----- Trail = {} -----\".format(t))\n",
    "    \n",
    "    # calc sowftMax\n",
    "    prob = sowftmax(beta,q_table)\n",
    "    if PRINT_EACH_TRAIL: print(\"Prob = {}\".format(prob))\n",
    "    \n",
    "    # choose an action according to sowftMax\n",
    "    action = np.random.choice([0,1] , p = prob)\n",
    "    action_list[t] = action\n",
    "    if PRINT_EACH_TRAIL: print(\"Action = {}\".format(action))\n",
    "    \n",
    "    # cheek if the trail is rewarded according to probs \n",
    "    reward = np.random.choice([0,1] , p = prob_reward[0]) if action == 0 \\\n",
    "                                                    else np.random.choice([0,1] , p = prob_reward[1])\n",
    "    \n",
    "    reward_list[t] = reward\n",
    "    if PRINT_EACH_TRAIL: print(\"Reward = {}\".format(reward))\n",
    "    \n",
    "    # update q_table according to q_learning formula\n",
    "    update_q_table(q_table,action,reward,alpha)\n",
    "    if PRINT_EACH_TRAIL: print(\"Q = {}\".format(q_table.tolist()))\n",
    "    if PRINT_EACH_TRAIL : input('PRESS ENTER FOR NEXT TRAIL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avarge rewared is = 0.884\n",
      "   action  reward\n",
      "0     0.0     1.0\n",
      "1     0.0     1.0\n",
      "2     0.0     0.0\n",
      "3     0.0     0.0\n",
      "4     0.0     0.0\n",
      "5     0.0     0.0\n",
      "6     0.0     0.0\n",
      "7     0.0     0.0\n",
      "8     0.0     0.0\n",
      "9     1.0     1.0\n"
     ]
    }
   ],
   "source": [
    "data = {'action' :action_list ,  'reward' : reward_list }\n",
    "df = pd.DataFrame(data,columns = ['action' ,'reward'])\n",
    "print(\"The avarge rewared is = {}\".format(df['reward'].mean()))\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-165-b5c73cb435a4>:19: RuntimeWarning: divide by zero encountered in log\n",
      "  ll = -(sum(np.log(p_choice)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[4.87011762, 0.19886181],\n",
       "       [4.87016486, 0.19888083],\n",
       "       [4.87008029, 0.19889725]]), array([172.08895765, 172.08895765, 172.08895774]))\n",
       "           fun: 172.08895764533668\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 105\n",
       "           nit: 54\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([4.87011762, 0.19886181])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def param_recovary_f(paramters, df):\n",
    "    \n",
    "    ll = 0 \n",
    "    q_table = np.zeros(2)\n",
    "\n",
    "    num_trials = len(df)\n",
    "    action = list(map(int, df['action']))\n",
    "    reward = list(map(int, df['reward'])) \n",
    "    p_choice= np.zeros(num_trials)\n",
    "    \n",
    "    beta = paramters[0]\n",
    "    alpha = paramters[1]\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        p_choice[t] = np.exp(beta * q_table[action[t]]) / sum(np.exp(beta*q_table))\n",
    "        p_e = reward[t] - q_table[action[t]]\n",
    "        q_table[action[t]] = q_table[action[t]] + alpha*p_e\n",
    "\n",
    "    ll = -(sum(np.log(p_choice)))\n",
    "    return ll\n",
    "\n",
    "def param_recovary(param):\n",
    "    return param_recovary_f(param,df)\n",
    "\n",
    "minimize(param_recovary,[1,0.5],method = 'Nelder-Mead')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
