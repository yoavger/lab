{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Step Task \n",
    " - This notebook simulate the two step task of a agent, store all the Data of the simulation and plot the Stay Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_frame(df): \n",
    "    # this funcation add to the data frame 3 columns:\n",
    "    # 1 - prev_reward : if the last trail was rewarded\n",
    "    # 2 - transation_prev : if the last transation was common or rare\n",
    "    # 3 - stay probs \n",
    "    \n",
    "    df['prev_reward'] = df['reward'].shift(1,fill_value=0)\n",
    "    df['transation_prev'] = df['transation_type'].shift(1,fill_value=0)\n",
    "    df['stay'] = df['action_statge_1'].shift(1) == df['action_statge_1']\n",
    "\n",
    "def calc_stay_probs(df): \n",
    "    # this funcation retuen a new df of stay prob according to every \n",
    "    # combination of reward/transation 4 option\n",
    "    \n",
    "    return df.groupby(['prev_reward', 'transation_prev'])['stay'].mean().reset_index()\n",
    "    \n",
    "def plot_stay_probs(list_of_stay):\n",
    "    # this funcation plots the stay probs given a list of the stay probs\n",
    "    # each element of the list represents combination of transation/reward\n",
    "    # list_of_stay[0] -> rare/unrewarded | list_of_stay [1] -> common/unrewarded\n",
    "    # list_of_stay[2] -> rare/rewarded | list_of_stay [3] -> common/rewarded\n",
    "    \n",
    "    x_labels = ['Rewarded' ,'Unrewarded']\n",
    "        \n",
    "    common = [list_of_stay[3],list_of_stay[1]]\n",
    "    rare = [list_of_stay[2],list_of_stay[0]]\n",
    "    \n",
    "    # the width of the bars\n",
    "    widthB = 0.35  \n",
    "    r1 = np.arange(len(x_labels))\n",
    "    r2 = [i + widthB for i in r1]\n",
    "    \n",
    "    fig , ax = plt.subplots()\n",
    "\n",
    "    ax.bar(r1,common, color='blue', width=widthB, edgecolor='white', label='Common')\n",
    "    ax.bar(r2, rare, color='red', width=widthB, edgecolor='white', label='Rare')\n",
    "\n",
    "    ax.set_ylabel('Stay Probability',size=12)\n",
    "    ax.set_title('PLOT',size=14)\n",
    "    ax.set_xticks([((2*r + widthB)/2) for r in range(len(x_labels))])\n",
    "    ax.set_xticklabels(x_labels,size=12)\n",
    "    ax.set_ylim((0, 1))  \n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return ax\n",
    "    \n",
    "def calc_main_effect(list_of_stay):\n",
    "     # this funcation retuen the main effect of stay prob\n",
    "    return ((list_of_stay[3] + list_of_stay[2])/2) - ((list_of_stay[1] +list_of_stay[0])/2)\n",
    "\n",
    "def calc_interaction_effect(list_of_stay):\n",
    "    # this funcation retuen the interaction effect of stay prob\n",
    "    return (list_of_stay[3] - list_of_stay[2]) - (list_of_stay[1] - list_of_stay[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOfSim():\n",
    "    \n",
    "    # this class stores all the data of one simulation\n",
    "    # storing the following: action_1, stage_2_state, transation_type, action_2, reward\n",
    "    def __init__ (self , num_of_trails):\n",
    "        self.action_1_list = np.zeros(num_of_trails)\n",
    "        self.stage_2_state = np.zeros(num_of_trails)\n",
    "        self.transation_list = ['' for i in range(num_of_trails)]\n",
    "        self.action_2_list = np.zeros(num_of_trails)\n",
    "        self.reward_list = np.zeros(num_of_trails) \n",
    "    \n",
    "    def createDic(self):\n",
    "        dic = {\n",
    "                'action_statge_1' : self.action_1_list,\n",
    "                'transation_type' : self.transation_list,\n",
    "                'action_stage_2' : self.action_2_list, \n",
    "                'state_of_stage_2' : self.stage_2_state,\n",
    "                'reward' : self.reward_list\n",
    "        }\n",
    "\n",
    "        return dic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of all parameters\n",
    "\n",
    "# set num of trails\n",
    "num_of_trails = 6000\n",
    "\n",
    "# set num of action \n",
    "action_space = 2\n",
    "\n",
    "# set num of states \n",
    "state_space = 2\n",
    "\n",
    "# set num of stages \n",
    "stages_space = 2\n",
    "\n",
    "# q_values of model free\n",
    "q_mf = np.zeros(shape = (action_space ,state_space, stages_space))\n",
    "\n",
    "# q_values of model based\n",
    "q_mb = np.zeros(action_space)\n",
    "\n",
    "# the weighted sum of model-based and model-free values\n",
    "q_net = np.zeros(2)\n",
    "\n",
    "# set learning rate - α  \n",
    "alpha_1 = 0.7\n",
    "\n",
    "# set discounting factor - γ \n",
    "gamma = 1\n",
    "\n",
    "# set beta temperature - β (for sowftMax)\n",
    "beta = 2\n",
    "\n",
    "# weighting parameter \n",
    "# w = 1 -> MB ; w = 0 -> MF\n",
    "w = 1\n",
    "\n",
    "# state transition structure \n",
    "transition_prob = np.array(\n",
    "                            [[.7,.3],\n",
    "                            [.3,.7]]\n",
    ")\n",
    "\n",
    "\n",
    "expected_reward = np.array(\n",
    "                            [[.8,.5],\n",
    "                            [.5,.8]]\n",
    ")\n",
    "\n",
    "# store data from each trail \n",
    "data = DataOfSim(num_of_trails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility funcation used for the q-learning\n",
    "\n",
    "def sowftmax(beta, q):\n",
    "    return (np.exp(q * beta)) / sum(np.exp(q * beta))\n",
    "\n",
    "def prediction_error(a , b): return a-b\n",
    "\n",
    "def update_q_table(q_table, reward, alpha, gamma = 1):\n",
    "    q_table[action] = q_table[action] + gamma * \\\n",
    "                                                (alpha * (prediction_error(reward,q_table[action])))\n",
    "def calculate_q_net(w, q_mb, q_mf):\n",
    "    return (w * q_mb) + ((1-w) * q_mf)\n",
    "\n",
    "def configuration_parameters():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to 1 to print step by step trail \n",
    "PRINT_EACH_TRAIL = 0\n",
    "\n",
    "for t in range(num_of_trails):\n",
    "    # this funcation simalate an agent playing the two step task num_of_trails times \n",
    "    # if PRINT_EACH_TRAIL is set to 1 this funcation will output evrey trail \n",
    "\n",
    "    if PRINT_EACH_TRAIL: print(\"----- Trail = {} -----\".format(t))\n",
    "\n",
    "    # Q_model-based values of the first level actions (Bellman’s equation)\n",
    "    q_mb[0] = (transition_prob[0,0]*np.max(q_mf[:,0,1])) + (transition_prob[0,1]*np.max(q_mf[:,1,1]))\n",
    "    q_mb[1] = transition_prob[1,0] * np.max(q_mf[:,0,1]) + transition_prob[1,1] * np.max(q_mf[:,1,1])\n",
    "    if PRINT_EACH_TRAIL: print(\"Q_MB = {}\".format(q_mb))\n",
    "\n",
    "    # net action values at the first stage as the weighted sum of model-based and model-free values\n",
    "    q_net[0] = calculate_q_net(w,q_mb[0],q_mf[0,0,0])\n",
    "    q_net[1] = calculate_q_net(w,q_mb[1],q_mf[1,0,0])\n",
    "\n",
    "    # calc prob with sowftMax for first stage\n",
    "    prob = sowftmax(beta,q_net)[0]\n",
    "    if PRINT_EACH_TRAIL: print(\"Prob 1 = {}\".format(prob))\n",
    "\n",
    "    # choose action_1 according to prob for first stage\n",
    "    action_1 = np.random.choice([0,1] , p = [prob,1-prob])\n",
    "    data.action_1_list[t] = action_1\n",
    "    if PRINT_EACH_TRAIL: print(\"Action 1 = {}\".format(action_1))\n",
    "\n",
    "    # transation to second stage \n",
    "    state = np.random.choice([0,1], p = [transition_prob[action_1,0], 1 - transition_prob[action_1,0]])\n",
    "    transation = 1 if action_1 == state else 0\n",
    "    data.stage_2_state[t] = state\n",
    "    data.transation_list[t] = 1 if transation == 1 else 0\n",
    "    if PRINT_EACH_TRAIL: print(\"Transation to State = {}\".format(state))\n",
    "    if PRINT_EACH_TRAIL: print(\"Transation type = {}\".format('Common' if transation == 1 else 'Rare'))\n",
    "\n",
    "    # calc prob with sowftMax for second stage\n",
    "    prob = sowftmax(beta,q_mf[:,state,1])[0]\n",
    "    if PRINT_EACH_TRAIL: print(\"Prob 2 = {}\".format(prob))\n",
    "\n",
    "    # choose action_2 according to prob for second stage\n",
    "    action_2 = np.random.choice([0,1] ,p = [prob, 1 - prob])\n",
    "    data.action_2_list[t] = action_2\n",
    "    if PRINT_EACH_TRAIL: print(\"Action 2 = {}\".format(action_1))\n",
    "\n",
    "    # cheek if the trail is rewarded\n",
    "    reward = np.random.choice([0,1] , p = [1-expected_reward[state,action_2], expected_reward[state,action_2]])\n",
    "    data.reward_list[t] = reward\n",
    "    if PRINT_EACH_TRAIL: print(\"Reward = {}\".format('YES!' if reward == 1 else 'NO'))\n",
    "\n",
    "    # calculate prediction error\n",
    "    p_e_1 = prediction_error(q_mf[action_2,state,1],q_mf[action_1,0,0])\n",
    "    p_e_2 = prediction_error(reward,q_mf[action_2,state,1])\n",
    "    if PRINT_EACH_TRAIL: print(\"Prediction Error first = {}\".format(p_e_1))\n",
    "    if PRINT_EACH_TRAIL: print(\"Prediction Error second = {}\".format(p_e_2))\n",
    "\n",
    "    # update q_mf according to q_learning formula\n",
    "    q_mf[action_1,0,0] = q_mf[action_1,0,0] + alpha_1 * p_e_1 + gamma * (alpha_1 * p_e_2)\n",
    "    q_mf[action_2,state,1] = q_mf[action_2,state,1] + alpha_1 * p_e_2\n",
    "\n",
    "    if PRINT_EACH_TRAIL: print(\"Q_MF = {}\".format(q_mf))\n",
    "    if PRINT_EACH_TRAIL : input('PRESS ENTER FOR NEXT TRAIL')\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data.createDic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avarge rewared is = 0.6958333333333333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_statge_1</th>\n",
       "      <th>transation_type</th>\n",
       "      <th>action_stage_2</th>\n",
       "      <th>state_of_stage_2</th>\n",
       "      <th>reward</th>\n",
       "      <th>prev_reward</th>\n",
       "      <th>transation_prev</th>\n",
       "      <th>stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_statge_1  transation_type  action_stage_2  state_of_stage_2  reward  \\\n",
       "0              0.0                1             0.0               0.0     1.0   \n",
       "1              0.0                0             0.0               1.0     1.0   \n",
       "2              0.0                1             0.0               0.0     1.0   \n",
       "3              1.0                1             0.0               1.0     1.0   \n",
       "4              1.0                0             0.0               0.0     1.0   \n",
       "\n",
       "   prev_reward  transation_prev   stay  \n",
       "0          0.0                0  False  \n",
       "1          1.0                1   True  \n",
       "2          1.0                0   True  \n",
       "3          1.0                1  False  \n",
       "4          1.0                1   True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The avarge rewared is = {}\".format(df['reward'].mean()))\n",
    "update_data_frame(df)\n",
    "stay_prob_df = calc_stay_probs(df)\n",
    "plot = plot_stay_probs(stay_prob_df['stay'].tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_recovary(paramters, df):\n",
    "    \n",
    "    ll = 0 \n",
    "    q_table = np.zeros(2)\n",
    "\n",
    "    num_trials = len(df)\n",
    "    action_1 = list(map(int, df['action_statge_1']))\n",
    "    action_2 = list(map(int, df['action_stage_2']))\n",
    "    reward = list(map(int, df['reward'])) \n",
    "    p_choice= np.zeros(num_trials)\n",
    "    \n",
    "    beta = paramters[0]\n",
    "    alpha = paramters[1]\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        p_choice[t] = np.exp(beta * q_table[action[t]]) / sum(np.exp(beta*q_table))\n",
    "        p_e = reward[t] - q_table[action[t]]\n",
    "        q_table[action[t]] = q_table[action[t]] + alpha*p_e\n",
    "\n",
    "    ll = -(sum(np.log(p_choice)))\n",
    "    return ll\n",
    "\n",
    "def fit_to_min(param):\n",
    "    return param_recovary(param,df)\n",
    "\n",
    "minimize(fit_to_min,[1,0.5],method = 'Nelder-Mead')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
